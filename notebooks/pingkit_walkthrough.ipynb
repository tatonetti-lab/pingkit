{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d6a815",
   "metadata": {},
   "source": [
    "# Pingkit End‑to‑End Modeling Walkthrough\n",
    "\n",
    "In this notebook we demonstrate a **minimal, fully‑reproducible pipeline** for training a probe on transformer embeddings with [Pingkit](https://github.com/tatonetti-lab/pingkit). We'll start from per‑token embeddings stored on disk, stack them into feature matrices, train an automatically‑sized MLP or CNN probe, and finally evaluate it on a held‑out test set.\n",
    "\n",
    "All the knobs you might want to tweak live in the next cell—feel free to edit them before running the rest of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c33f3",
   "metadata": {},
   "source": [
    "## 0. Editable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ╒══════════════════════════════════════════════════════════════╕\n",
    "# │                          PARAMETERS                          │\n",
    "# ╘══════════════════════════════════════════════════════════════╛\n",
    "# Embedding configuration\n",
    "LAYER             = 39                # Transformer layer to use\n",
    "EMB_PARTS         = \"rs\"              # 'rs', 'attn', 'mlp', or a list like ['rs','mlp']\n",
    "\n",
    "# Paths\n",
    "TRAIN_EMB_DIR     = \"safety_train\"    # Directory of per‑row embedding CSVs\n",
    "TEST_EMB_DIR      = \"safety_test\"\n",
    "TRAIN_LABEL_CSV   = \"safety_train.csv\"  # CSV with columns ['id', 'label']\n",
    "TEST_LABEL_CSV    = \"safety_test.csv\"\n",
    "\n",
    "# Training hyper‑parameters\n",
    "MODEL_TYPE        = \"mlp\"             # 'mlp' or 'cnn'\n",
    "BATCH_SIZE        = 128\n",
    "N_EPOCHS          = 100\n",
    "LEARNING_RATE     = 1e-3\n",
    "RANDOM_STATE      = 405\n",
    "DEVICE            = \"cuda\"            # 'cuda' or 'cpu'\n",
    "\n",
    "# Where to store the trained model\n",
    "ARTIFACT_ROOT     = f\"artifacts/run_L{LAYER}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d2df6",
   "metadata": {},
   "source": [
    "The notebook will create directories as needed—no manual setup required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664228fc",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "First, let's gather the standard libraries and the high‑level Pingkit helpers we'll be using throughout the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pathlib, json, textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pingkit.embedding  import embed_dataset\n",
    "from pingkit.extraction import extract_token_vectors\n",
    "from pingkit.model      import (\n",
    "    fit, save_artifacts, load_npz_features,\n",
    "    _evaluate, load_artifacts\n",
    ")\n",
    "\n",
    "from sklearn.metrics     import accuracy_score, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee0098",
   "metadata": {},
   "source": [
    "## 2. Stack per‑row embeddings into a feature matrix\n",
    "Pingkit's `extract_token_vectors` scans an embeddings directory (**one file per row per layer** as produced by `embed_dataset`) and concatenates everything into a single compressed `.npz`. We do this separately for the training and test splits—**run this section only once** after generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_npz = extract_token_vectors(\n",
    "    str(TRAIN_EMB_DIR),\n",
    "    output_file=f\"{TRAIN_EMB_DIR}/results/features_{EMB_PARTS}_L{LAYER}.npz\",\n",
    "    layers=LAYER,\n",
    "    parts=EMB_PARTS,\n",
    "    n_jobs=os.cpu_count(),\n",
    ")\n",
    "print(\"✅  Stacked train features:\", train_npz)\n",
    "\n",
    "test_npz = extract_token_vectors(\n",
    "    str(TEST_EMB_DIR),\n",
    "    output_file=f\"{TEST_EMB_DIR}/results/features_{EMB_PARTS}_L{LAYER}.npz\",\n",
    "    layers=LAYER,\n",
    "    parts=EMB_PARTS,\n",
    "    n_jobs=os.cpu_count(),\n",
    ")\n",
    "print(\"✅  Stacked test  features:\", test_npz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041428b",
   "metadata": {},
   "source": [
    "## 3. Load features and labels\n",
    "Next we load the `.npz` files into Pandas DataFrames, read the label CSVs, and **align** the indices so that every row has both features and a ground‑truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── Features ───────────────────────────────────────────────────────\n",
    "X_train_df, meta = load_npz_features(f\"{TRAIN_EMB_DIR}/results/features_{EMB_PARTS}_L{LAYER}.npz\")\n",
    "X_test_df,  _    = load_npz_features(f\"{TEST_EMB_DIR}/results/features_{EMB_PARTS}_L{LAYER}.npz\")\n",
    "\n",
    "# ── Labels ─────────────────────────────────────────────────────────\n",
    "y_train = pd.read_csv(TRAIN_LABEL_CSV, index_col='id')['label']\n",
    "y_test  = pd.read_csv(TEST_LABEL_CSV,  index_col='id')['label']\n",
    "\n",
    "# Align indices (drop rows missing on either side)\n",
    "common_train = X_train_df.index.intersection(y_train.index)\n",
    "X_train_df   = X_train_df.loc[common_train]\n",
    "y_train      = y_train.loc[common_train]\n",
    "\n",
    "common_test  = X_test_df.index.intersection(y_test.index)\n",
    "X_test_df    = X_test_df.loc[common_test]\n",
    "y_test       = y_test.loc[common_test]\n",
    "\n",
    "print(f\"Train set: {X_train_df.shape}, labels: {y_train.shape}\")\n",
    "print(f\"Test  set: {X_test_df.shape},  labels: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e85643",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "We call Pingkit's `fit` helper, which will:\n",
    "1. Infer a reasonable network width based on the number of training examples.\n",
    "2. Use an internal 20 % validation split for early stopping.\n",
    "3. Return both the trained model and a per‑epoch history we could plot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0534ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, history = fit(\n",
    "    X_train_df,\n",
    "    y_train.values,\n",
    "    model_type     = MODEL_TYPE,\n",
    "    meta           = meta,\n",
    "    num_classes    = len(np.unique(y_train)),\n",
    "    metric         = 'loss',\n",
    "    batch_size     = BATCH_SIZE,\n",
    "    learning_rate  = LEARNING_RATE,\n",
    "    n_epochs       = N_EPOCHS,\n",
    "    val_split      = 0.2,\n",
    "    patience       = 10,\n",
    "    early_stopping = True,\n",
    "    random_state   = RANDOM_STATE,\n",
    "    device         = DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324453c",
   "metadata": {},
   "source": [
    "## 5. Save trained artifacts\n",
    "To make future inference trivial, we persist both the network weights **and** the metadata that tells Pingkit exactly how to reconstruct the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ab544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_path, meta_path = save_artifacts(\n",
    "    model,\n",
    "    path=str(ARTIFACT_ROOT),\n",
    "    meta=meta,\n",
    ")\n",
    "print(\"Model weights saved to :\", weights_path)\n",
    "print(\"Metadata saved to      :\", meta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75a051",
   "metadata": {},
   "source": [
    "## 6. Evaluate on the test split\n",
    "We reload the saved model (just to prove that serialization works) and compute accuracy and ROC‑AUC on the hold‑out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, _ = load_artifacts(str(ARTIFACT_ROOT), device=DEVICE)\n",
    "\n",
    "device    = next(model.parameters()).device\n",
    "X_test_np = X_test_df.values.astype(np.float32)\n",
    "\n",
    "probs, _, _ = _evaluate(\n",
    "    model,\n",
    "    X_test_np,\n",
    "    y_test.values,\n",
    "    model_type = MODEL_TYPE,\n",
    "    metric_fn  = lambda y, p: accuracy_score(y, p.argmax(1)),\n",
    "    device     = device,\n",
    ")\n",
    "\n",
    "pred_labels = probs.argmax(1)\n",
    "acc = accuracy_score(y_test.values, pred_labels)\n",
    "\n",
    "# Binary vs. multiclass AUC handling\n",
    "if probs.shape[1] == 2:\n",
    "    auc = roc_auc_score(y_test.values, probs[:,1])\n",
    "else:\n",
    "    auc = roc_auc_score(y_test.values, probs, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"AUC     : {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691a085",
   "metadata": {},
   "source": [
    "## 7. Calibration curve (optional)\n",
    "If we're curious about how well‑calibrated the model's probabilities are, we can draw a quick reliability diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prob_pos = probs[:, 1] if probs.shape[1] == 2 else probs.max(1)\n",
    "frac_pos, mean_pred = calibration_curve(y_test.values, prob_pos, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(mean_pred, frac_pos, marker='o', linewidth=1.5, label='Model')\n",
    "plt.plot([0,1],[0,1], linestyle='--', label='Perfect')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d1057",
   "metadata": {},
   "source": [
    "That's it—we've **gone from stacked transformer embeddings to a trained, saved, and evaluated probe** in just a few steps. Try experimenting with different layers, embedding parts, or model architectures by tweaking the parameters at the top—Pingkit will handle the rest. Happy probing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
